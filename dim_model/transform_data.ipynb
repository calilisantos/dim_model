{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7137a415",
   "metadata": {},
   "source": [
    "# Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ce233b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de03a0a7",
   "metadata": {},
   "source": [
    "# Create a dataframe list with sheets values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc0faf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "months_list = [\n",
    "    \"JANEIRO\",\n",
    "    \"FEVEREIRO\",\n",
    "    \"MARÇO\",\n",
    "    \"ABRIL\",\n",
    "    \"MAIO\",\n",
    "    \"JUNHO\",\n",
    "    \"JULHO\",\n",
    "    \"AGOSTO\",\n",
    "    \"SETEMBRO\",\n",
    "    \"OUTUBRO\",\n",
    "    \"NOVEMBRO\",\n",
    "    \"DEZEMBRO\"\n",
    "]\n",
    "\n",
    "# Lista para armazenar os dados de todas as planilhas\n",
    "# data = list()\n",
    "# Loop para percorrer os arquivos de 2016 a 2023\n",
    "for year in range(2016, 2023):\n",
    "    arquivo_excel = f\"sheets/CONTROLE CHEQUE {year}.xlsx\"  # Nome do arquivo Excel para cada ano\n",
    "    arquivo_excel_destino = f\"sheets/CONTROLE CHEQUE {year}_mask.xlsx\"\n",
    "    # Loop para percorrer as abas de cada arquivo\n",
    "    with pd.ExcelWriter(arquivo_excel_destino) as writer:  # doctest: +SKIP\n",
    "        for month in months_list:  \n",
    "            try:\n",
    "                # Ler a planilha atual e adicionar os dados à lista\n",
    "                df = pd.read_excel(arquivo_excel, sheet_name=month)\n",
    "                columns_to_clean =['CLIENTE', 'OBSERVAÇÃO', 'DESTINO']\n",
    "                df['Nº CHEQUE'] = df['Nº CHEQUE'].astype('category').cat.codes + 1\n",
    "\n",
    "                for coluna in columns_to_clean:\n",
    "                    df[coluna] = df[coluna].str[0] + df[coluna].str[-1]\n",
    "                \n",
    "                df.drop(['Unnamed: 2', 'Unnamed: 3'], axis=1).to_excel(writer, sheet_name=month, index=False)\n",
    "\n",
    "                # df.to_excel(arquivo_excel_destino, index=False, sheet_name=month)\n",
    "            except:\n",
    "                print(f\"Mês {month} de {year} vazio ou não existe na planilha\")\n",
    "# 2023 Unnamed: 11 a 15, outros anos Unnamed: 2 e 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a7272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivo_excel = f\"sheets/CONTROLE CHEQUE 2023.xlsx\"  # Nome do arquivo Excel para cada ano\n",
    "arquivo_excel_destino = f\"sheets/CONTROLE CHEQUE 2023_mask.xlsx\"\n",
    "# Loop para percorrer as abas de cada arquivo\n",
    "with pd.ExcelWriter(arquivo_excel_destino) as writer:  # doctest: +SKIP\n",
    "    for month in months_list:  \n",
    "        try:\n",
    "            # Ler a planilha atual e adicionar os dados à lista\n",
    "            df = pd.read_excel(arquivo_excel, sheet_name=month)\n",
    "            columns_to_clean =['CLIENTE', 'OBSERVAÇÃO', 'DESTINO']\n",
    "            df['Nº CHEQUE'] = df['Nº CHEQUE'].astype('category').cat.codes + 1\n",
    "\n",
    "            for coluna in columns_to_clean:\n",
    "                df[coluna] = df[coluna].str[0] + df[coluna].str[-1]\n",
    "            \n",
    "            df.drop(['Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14', 'Unnamed: 15'], axis=1).to_excel(writer, sheet_name=month, index=False)\n",
    "\n",
    "            # df.to_excel(arquivo_excel_destino, index=False, sheet_name=month)\n",
    "        except:\n",
    "            print(f\"Mês {month} de {year} vazio ou não existe na planilha\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fcf589",
   "metadata": {},
   "source": [
    "# Merge dataframe list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c4564b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenar os dados de todas as planilhas em um único DataFrame\n",
    "concat_data = pd.concat(data)\n",
    "\n",
    "concat_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8b734c",
   "metadata": {},
   "source": [
    "# Config raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9117630",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_columns_list = [\n",
    "    \"CLIENTE\",\n",
    "    \"BANCO\",\n",
    "    \"Nº CHEQUE\",\n",
    "    \"VALOR (R$)\",\n",
    "    \"VENCIMENTO\",\n",
    "    \"SAÍDA\",\n",
    "    \"COD\",\n",
    "    \"DESTINO\",\n",
    "    \"OBSERVAÇÃO\"\n",
    "]\n",
    "\n",
    "\n",
    "REQUIRED_COLUMNS = 5 # CLIENTE, BANCO, Nº CHEQUE, VALOR (R$), VENCIMENTO\n",
    "\n",
    "raw_data = concat_data[raw_columns_list].dropna(thresh=REQUIRED_COLUMNS)\n",
    "db_user = 'root'\n",
    "db_name = 'Checks'\n",
    "db_password = '123456'\n",
    "db_host = 'localhost'\n",
    "engine = create_engine(f'mysql+mysqlconnector://{db_user}:{db_password}@{db_host}/{db_name}')\n",
    "\n",
    "raw_table_name = 'raw_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55df0e57",
   "metadata": {},
   "source": [
    "# Save raw_data in SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d644bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use o método to_sql para salvar o DataFrame no banco de dados MySQL\n",
    "raw_data.to_sql(\n",
    "    raw_table_name,\n",
    "    con=engine,\n",
    "    index=True,\n",
    "    if_exists='replace'\n",
    ")\n",
    "\n",
    "# print(f'Dados salvos na tabela {raw_table_name} no banco de dados MySQL.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a7c3a7",
   "metadata": {},
   "source": [
    "# Get raw_data info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4f331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd1723c",
   "metadata": {},
   "source": [
    "# Drop null values if CLIENTE and Nº Cheque is null\n",
    "# IMPORTANTE: \n",
    "* ## adicionar COD? \n",
    "* ## **DECISÃO**: Não, registros novos não tem cod. Um caso de registro antigo sem cod corrigido ao longo do caminho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c644bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop null values if CLIENTE or Nº Cheque is null\n",
    "accur_data = raw_data.dropna(subset=['CLIENTE', 'BANCO'], how='any')\n",
    "\n",
    "accur_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7607baa",
   "metadata": {},
   "source": [
    "# Drop duplicates values if CLIENTE and Nº Cheque are equal\n",
    "## IMPORTANTE: \n",
    "* # Será que não é bom ter duplicadas nessa situação para testar a tabela fato?\n",
    "* ## **DECISÃO**: Por enquanto sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32b1274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropar duplicatas se cliente, banco e nº cheque forem iguais, mantendo a que tiver valor na coluna saída\n",
    "accur_data = accur_data.drop_duplicates(subset=['CLIENTE', 'BANCO', 'Nº CHEQUE', 'SAÍDA'], keep='last')\n",
    "accur_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c71e341",
   "metadata": {},
   "source": [
    "# Transform BANCO column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8621541e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accur_data['BANCO'].unique()\n",
    "\n",
    "# array(['ITAU', 'BB', 'CAIXA', 'BNB', 'SICOOB', 'BRADESCO', 'BD',\n",
    "#        'SANTANDER', 'itau', 'SICOB', 'UNICRED', ' BD', 'HSBC',\n",
    "#        'ITAU       ', 'BB ', 'BANESE', 'SICREDI', 'CITIBANK', 'ASCOOB',\n",
    "#        'SICRED', 'SAFRA', 'SICREDII', 'BS'], dtype=object)\n",
    "\n",
    "banks_adjust_dict = {\n",
    "    'BB': 'BANCO DO BRASIL',\n",
    "    'BNB': 'BANCO DO NORDESTE',\n",
    "    'SICOB': 'SICOOB',\n",
    "    'BD': 'BRADESCO',\n",
    "    'BS': 'BRADESCO',\n",
    "    'itau': 'ITAU',\n",
    "    'BD': 'BRADESCO',\n",
    "    'SICRED': 'SICREDI',\n",
    "    'SICREDII': 'SICREDI'\n",
    "}\n",
    "# Corrigir espaços em branco extras e typos nos nomes dos bancos\n",
    "accur_data['BANCO'] = accur_data['BANCO'].str.strip()  # Remove espaços em branco no início e no final\n",
    "accur_data['BANCO'] = accur_data['BANCO'].replace(banks_adjust_dict)  # Substitua os nomes dos bancos com o dicionário\n",
    "\n",
    "# Agora, você pode verificar os valores únicos novamente\n",
    "print(accur_data['BANCO'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5f0515",
   "metadata": {},
   "source": [
    "# Transform CLIENTE column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6f9c32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a02e6779",
   "metadata": {},
   "source": [
    "# Transform COD column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d31a68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(accur_data['COD'].unique())\n",
    "\n",
    "# ['P' 'D' 'T' 'O' 'p' 'P ' nan ' P']\n",
    "cod_dict = {\n",
    "    'P': 'PAGAMENTO',\n",
    "    'D': 'DEPOSITO',\n",
    "    'O': 'OUTROS',\n",
    "    'T': 'CAIXA'\n",
    "}\n",
    "\n",
    "accur_data['COD'] = accur_data['COD'].str.upper()  # Converta todos os valores para maiúsculas\n",
    "accur_data['COD'] = accur_data['COD'].str.strip()  # Remova espaços em branco no início e no final\n",
    "# substituir 'P' por 'PAGO', 'D' por 'DEPOSITO' e 'O' por 'OUTROS'\n",
    "accur_data['COD'] = accur_data['COD'].replace(cod_dict)\n",
    "# Substitua os valores nulos por 'caixa'\n",
    "accur_data['COD'] = accur_data['COD'].fillna('CAIXA')\n",
    "print(accur_data['COD'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8138c41e",
   "metadata": {},
   "source": [
    "# Transform DESTINO column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3200852a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94d184f2",
   "metadata": {},
   "source": [
    "# Transform VALOR (R$) column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8634f71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accur_data['VALOR (R$)'] = accur_data['VALOR (R$)'].astype(str).str.replace('.', '').str.replace(',', '.').astype(float)\n",
    "accur_data['VALOR (R$)'] = accur_data['VALOR (R$)'].astype(str).str.replace(',', '.').astype(float)\n",
    "accur_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b224871",
   "metadata": {},
   "source": [
    "# Transform date columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc166edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_columns_list = [\n",
    "    'VENCIMENTO',\n",
    "    'SAÍDA'\n",
    "]\n",
    "\n",
    "replace_dict = {\n",
    "    r'\\s+': '',\n",
    "    '00:00:00': '', \n",
    "    '29/2/2017': '2017-02-28',\n",
    "    '29/2/2018': '2018-02-28',\n",
    "    '29/2/2019': '2019-02-28',\n",
    "    '31/4/2017': '2017-04-30', \n",
    "    '31/4/2018': '2018-04-30', \n",
    "    '30/62017': '2017-06-30', \n",
    "    '31/9/2017': '2017-09-30', \n",
    "    '2017-09-31': '2017-09-30', \n",
    "    '31/6/2018': '2018-06-30', \n",
    "    '31/9/2018': '2018-09-30',\n",
    "    ',15/04/2016': '2016-04-15',\n",
    "    '15/04/2016': '2016-04-15',\n",
    "    '15/04/201': '2016-04-15',\n",
    "    '10/06/2016': '2016-06-10',\n",
    "    # '25/07/216': '2016-07-25',\n",
    "    # '22/08/216': '2016-08-22',\n",
    "    # '21/09/216': '2016-09-21',\n",
    "    # '17/10/216': '2016-10-17',\n",
    "    '216': '2016',\n",
    "    '2216': '2016',\n",
    "    '217': '2017',\n",
    "    '218': '2018',\n",
    "    '219': '2019',\n",
    "    '221': '2021',\n",
    "    '10/112017': '2017-11-10',\n",
    "    '227/9/2018': '2018-09-27',\n",
    "    # '22016-10-07': '2016-10-07',\n",
    "}\n",
    "\n",
    "accur_data = reduce(\n",
    "    lambda df, column: df.assign(**{\n",
    "        column: df[column].astype(str).replace(replace_dict, regex=True)\n",
    "    }),\n",
    "    date_columns_list,\n",
    "    accur_data\n",
    ")\n",
    "\n",
    "accur_data[date_columns_list].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fa6b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accur_data.iloc[87]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d328342",
   "metadata": {},
   "outputs": [],
   "source": [
    "accur_data = reduce(\n",
    "    lambda df, column: df.assign(**{\n",
    "        column: pd.to_datetime(df[column], format='mixed')#.dt.strftime('%d/%m/%Y')\n",
    "    }),\n",
    "    date_columns_list,\n",
    "    accur_data\n",
    ")\n",
    "\n",
    "accur_data[date_columns_list].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31592f5-41ea-4f2d-9e77-f307e7d6319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accur_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18c8e1e-2fe4-4c91-bfc0-83e7e0cf8383",
   "metadata": {},
   "source": [
    "# Add is_in_cash column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895d2401-4644-4872-89c9-f43061bca2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar coluna booleana is_in_cash sendo false quando o valor da coluna SAÍDA não for nulo, Todos os registros com o mesmo valor para as colunas CLIENTE, BANCO, Nº CHEQUE, VALOR (R$), VENCIMENTO terão o valor FALSE na coluna is_in_cash\n",
    "\n",
    "common_columns_list = [\n",
    "    'CLIENTE',\n",
    "    'BANCO',\n",
    "    'Nº CHEQUE',\n",
    "    'VALOR (R$)'\n",
    "]\n",
    "\n",
    "checks_out = accur_data[accur_data['SAÍDA'].notnull()][common_columns_list]\n",
    "# accur_data['is_in_cash'] = np.where(\n",
    "#     accur_data[['CLIENTE', 'BANCO', 'Nº CHEQUE', 'VALOR (R$)', 'VENCIMENTO']].isin(checks_out).all(axis=1),\n",
    "#     False,\n",
    "#     True\n",
    "# )\n",
    "\n",
    "# checks_out.head(30)\n",
    "\n",
    "# accur_data['is_in_cash'] = ~accur_data[common_columns_list].isin(checks_out.to_dict()).all(axis=1)\n",
    "\n",
    "accur_data['is_in_cash'] = np.where(\n",
    "    accur_data['CLIENTE'].isin(checks_out['CLIENTE']) &\n",
    "    accur_data['BANCO'].isin(checks_out['BANCO']) &\n",
    "    accur_data['Nº CHEQUE'].isin(checks_out['Nº CHEQUE']) &\n",
    "    accur_data['VALOR (R$)'].isin(checks_out['VALOR (R$)']),\n",
    "    False,\n",
    "    True\n",
    ")\n",
    "\n",
    "# accur_data['is_in_cash'] = reduce(\n",
    "#     lambda df, column: df.assign(**{\n",
    "#         column: np.where(\n",
    "#             df[column].isin(checks_out[column]),\n",
    "#             False,\n",
    "#             True\n",
    "#         )\n",
    "#     }),\n",
    "#     common_columns_list,\n",
    "#     accur_data\n",
    "# )\n",
    "\n",
    "# Substitui valores nulos em 'is_in_cash' por True\n",
    "# accur_data['is_in_cash'].fillna(True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b957a839",
   "metadata": {},
   "source": [
    "# Transform Nº CHEQUE column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f42e0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accur_data['Nº CHEQUE'] = accur_data['Nº CHEQUE'].astype(int)\n",
    "# accur_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49567101",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.to_excel('raw_data.xlsx', index=False)\n",
    "accur_data.to_excel('accur_data.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaebabb",
   "metadata": {},
   "source": [
    "# Save accur_data in SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1d034e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use o método to_sql para salvar o DataFrame no banco de dados MySQL\n",
    "accur_table_name = 'accur_data'\n",
    "accur_data.to_sql(\n",
    "    accur_table_name,\n",
    "    con=engine,\n",
    "    index=True,\n",
    "    if_exists='replace'\n",
    ")\n",
    "\n",
    "print(f'Dados salvos na tabela {accur_table_name} no banco de dados MySQL.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d5eae4",
   "metadata": {},
   "source": [
    "# Create primary keys dimensional models columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5b288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_keys_columns = [\n",
    "    \"CLIENTE\",\n",
    "    \"BANCO\",\n",
    "    # \"DESTINO\",\n",
    "    \"COD\"\n",
    "    # \"OBSERVAÇÃO\"\n",
    "]\n",
    "\n",
    "# criar uma coluna para o id do nome considerando o nome do cliente. Nome do cliente pode se repetir\n",
    "# raw_data['name_id'] = raw_data['CLIENTE'].astype('category').cat.codes + 1\n",
    "\n",
    "# criar uma coluna para o id das primary_keys_columns. Valores dos campos podem se repetir\n",
    "model_data = reduce(\n",
    "    lambda df, col: df.assign(**{f'{col.lower()}_id': df[col].astype('category').cat.codes + 1}),\n",
    "    primary_keys_columns,\n",
    "    accur_data\n",
    ")\n",
    "\n",
    "model_data.info()\n",
    "print(model_data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f76e807",
   "metadata": {},
   "source": [
    "# Add name_dim columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c12a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE TABLE name_dim(\n",
    "# \tname_id INT NOT NULL IDENTITY(1,1) PRIMARY KEY,\n",
    "# \t\"name\" VARCHAR(100) NOT NULL,\n",
    "# \tis_client BOOL NOT NULL DEFAULT true,\n",
    "# \tclient_name VARCHAR(100) DEFAULT \"name\"\n",
    "# );\n",
    "name_dim_columns = [\n",
    "    'name_id',\n",
    "    'name',\n",
    "    'is_client',\n",
    "    'client_name'\n",
    "]\n",
    "\n",
    "# criar coluna is_client se valor de OBSERVAÇÃO for vazio ou se o valor de CLIENTE estar em OBSERVAÇÃO\n",
    "model_data['is_client'] = np.where(model_data['OBSERVAÇÃO'].isnull() | model_data['CLIENTE'].isin(model_data['OBSERVAÇÃO']), True, False)\n",
    "\n",
    "# renomear: client_id para name_id e CLIENTE para name\n",
    "new_name_dim_columns = {\n",
    "    'CLIENTE': 'name',\n",
    "    'cliente_id': 'name_id'\n",
    "}\n",
    "model_data = model_data.rename(columns=new_name_dim_columns)\n",
    "\n",
    "# criar coluna client_name com o valor de name se is_client for true. Se for falso mas o valor de OBSERVAÇÃO estiver na coluna CLIENTE, então o valor de client_name será o valor de OBSERVAÇÃO. Ao contrário deixar vazio\n",
    "model_data['client_name'] = np.where(\n",
    "    model_data['is_client'] == True, model_data['name'], \n",
    "        np.where(\n",
    "            model_data['OBSERVAÇÃO'].isin(model_data['name']) & model_data['is_client'] == False, model_data['OBSERVAÇÃO'],\n",
    "            ''\n",
    "        )\n",
    ")\n",
    "\n",
    "model_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59e0bbc",
   "metadata": {},
   "source": [
    "# Add bank_dim columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001d9def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE TABLE bank_dim(\n",
    "# \tbank_id INT NOT NULL IDENTITY(1,1) PRIMARY KEY,\n",
    "# \tbank_name VARCHAR(100) NOT NULL\n",
    "# );\n",
    "bank_dim_columns = [\n",
    "    'bank_id',\n",
    "    'bank_name'\n",
    "]\n",
    "\n",
    "new_bank_dim_columns = {\n",
    "    'BANCO': 'bank_name',\n",
    "    'banco_id': 'bank_id'\n",
    "}\n",
    "model_data = model_data.rename(columns=new_bank_dim_columns)\n",
    "\n",
    "model_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e837db81",
   "metadata": {},
   "source": [
    "# Add check_dim columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6956d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE TABLE check_dim(\n",
    "# \tcheck_id INT NOT NULL IDENTITY(1,1) PRIMARY KEY,\n",
    "# \tcheck_number INT NOT NULL,\n",
    "# \tcheck_amount FLOAT NOT NULL,\n",
    "# \tmaturity_date DATE NOT NULL\n",
    "# );\n",
    "check_dim_columns = [\n",
    "    'check_id',\n",
    "    'check_number',\n",
    "    'check_amount',\n",
    "    'maturity_date'\n",
    "]\n",
    "\n",
    "new_check_dim_columns = {\n",
    "    'Nº CHEQUE': 'check_number',\n",
    "    'VALOR (R$)': 'check_amount',\n",
    "    'VENCIMENTO': 'maturity_date'\n",
    "}\n",
    "\n",
    "model_data = model_data.rename(columns=new_check_dim_columns)\n",
    "\n",
    "# criando coluna check_id para cada linha única de check_number, name, bank_name\n",
    "model_data['check_id'] = (\n",
    "    model_data.groupby(\n",
    "        [\n",
    "            # 'maturity_date',\n",
    "            'name',\n",
    "            'check_number',\n",
    "            'bank_name',\n",
    "            'check_amount'\n",
    "        ]\n",
    "    )\n",
    "        .ngroup() + 1\n",
    "\n",
    ")\n",
    "\n",
    "# model_data['check_id'] = (\n",
    "#     model_data.maturity_date.sort_values()\n",
    "#         .str.cat(\n",
    "#             model_data.name.str.cat(\n",
    "#                 model_data.check_number.astype(str).str.cat(\n",
    "#                     model_data.bank_name.str.cat(\n",
    "#                         model_data.check_amount.astype(str), \n",
    "#                         sep='-'\n",
    "#                     ), \n",
    "#                     sep='-'\n",
    "#                 ), \n",
    "#                 sep='-'\n",
    "#             ), \n",
    "#             sep='-'\n",
    "#         )\n",
    "#             .astype('category').cat.codes + 1\n",
    "# )\n",
    "\n",
    "model_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7e74c3",
   "metadata": {},
   "source": [
    "# Add dest_dim columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347b4a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE TABLE dest_dim(\n",
    "# \tdestination_id INT NOT NULL IDENTITY(1,1) PRIMARY KEY,\n",
    "# \tdestination_name VARCHAR(100) NOT NULL,\n",
    "# \tdestination_type VARCHAR(100) NOT NULL  DEFAULT \"CAIXA\",\n",
    "# \tdestination_complement VARCHAR(100)\n",
    "# )\n",
    "dest_dim_columns = [\n",
    "    'destination_id',\n",
    "    # 'destination_name',\n",
    "    'destination_type',\n",
    "    # 'destination_complement'\n",
    "]\n",
    "\n",
    "new_dest_dim_columns = {\n",
    "    # 'DESTINO': 'destination_name',\n",
    "    'cod_id': 'destination_id',\n",
    "    'COD': 'destination_type',\n",
    "    # 'OBSERVAÇÃO': 'destination_complement'\n",
    "}\n",
    "\n",
    "model_data = model_data.rename(columns=new_dest_dim_columns)\n",
    "model_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cae5aa",
   "metadata": {},
   "source": [
    "# Transform SAÍDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674eb73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data['SAÍDA'] = np.where(\n",
    "    model_data['SAÍDA'].isnull(),\n",
    "    model_data['maturity_date'],\n",
    "    model_data['SAÍDA']\n",
    ")\n",
    "\n",
    "model_data = model_data.rename(columns={'SAÍDA': 'registration_date'})\n",
    "model_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c3885e",
   "metadata": {},
   "source": [
    "# Transform DESTINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4abf99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data['description'] = model_data['DESTINO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f88cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data.drop(columns=['DESTINO', 'OBSERVAÇÃO']).to_excel('model_data.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649ece1c",
   "metadata": {},
   "source": [
    "# Create dim & fact tables\n",
    "# TO DO:\n",
    "# **Validar check_id**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f763ce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE TABLE check_fact(\n",
    "# \tid INT NOT NULL IDENTITY(1,1) PRIMARY KEY,\n",
    "# \tFOREIGN KEY(name_id) REFERENCES name_dim(name_id),\n",
    "# \tFOREIGN KEY(bank_id) REFERENCES bank_dim(bank_id),\n",
    "# \tFOREIGN KEY(check_id) REFERENCES check_dim(check_id),\n",
    "# \tFOREIGN KEY(destination_id) REFERENCES name_dim(dest_dim),\n",
    "# \t\"registration_date\" DATE NOT NULL\n",
    "# )\n",
    "\n",
    "# criar coluna id para cada linha de model_data\n",
    "model_data['id'] = model_data.index + 1\n",
    "check_fact_columns = [\n",
    "    'id',\n",
    "    'name_id',\n",
    "    'bank_id',\n",
    "    'check_id',\n",
    "    'destination_id',\n",
    "    'is_in_cash',\n",
    "    'description',\n",
    "    'registration_date'\n",
    "]\n",
    "\n",
    "# criar tabelas dimensões e fatos com os valores de model_data\n",
    "name_dim = model_data[name_dim_columns].drop_duplicates()\n",
    "bank_dim = model_data[bank_dim_columns].drop_duplicates()\n",
    "check_dim = model_data[check_dim_columns].drop_duplicates()\n",
    "dest_dim = model_data[dest_dim_columns].drop_duplicates()\n",
    "check_fact = model_data[check_fact_columns]#.drop_duplicates()\n",
    "# continua..\n",
    "name_dim.to_sql(\n",
    "    'name_dim',\n",
    "    con=engine,\n",
    "    index=False,\n",
    "    if_exists='replace'\n",
    ")\n",
    "\n",
    "bank_dim.to_sql(\n",
    "    'bank_dim',\n",
    "    con=engine,\n",
    "    index=False,\n",
    "    if_exists='replace'\n",
    ")\n",
    "\n",
    "check_dim.to_sql(\n",
    "    'check_dim',\n",
    "    con=engine,\n",
    "    index=False,\n",
    "    if_exists='replace'\n",
    ")\n",
    "\n",
    "dest_dim.to_sql(\n",
    "    'dest_dim',\n",
    "    con=engine,\n",
    "    index=False,\n",
    "    if_exists='replace'\n",
    ")\n",
    "\n",
    "check_fact.to_sql(\n",
    "    'check_fact',\n",
    "    con=engine,\n",
    "    index=False,\n",
    "    if_exists='replace'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969e89a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
